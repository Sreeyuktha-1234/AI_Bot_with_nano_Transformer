{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "with open('Myfile.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "IrYpG_e4pVUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYHFDGGjpgi8",
        "outputId": "783c50b5-7dca-489c-be41-b88fba69493b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  28250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:4485])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Pvs40B1pjDl",
        "outputId": "0e68cac0-6e50-475d-8496-1ae83291d71d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is artificial intelligence (AI)?\n",
            "Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. Specific applications of AI include expert systems, natural language processing, speech recognition and machine vision.\n",
            "\n",
            "How does AI work?\n",
            "As the hype around AI has accelerated, vendors have been scrambling to promote how their products and services use it. Often, what they refer to as AI is simply a component of the technology, such as machine learning. AI requires a foundation of specialized hardware and software for writing and training machine learning algorithms. No single programming language is synonymous with AI, but Python, R, Java, C++ and Julia have features popular with AI developers.\n",
            "\n",
            "In general, AI systems work by ingesting large amounts of labeled training data, analyzing the data for correlations and patterns, and using these patterns to make predictions about future states. In this way, a chatbot that is fed examples of text can learn to generate lifelike exchanges with people, or an image recognition tool can learn to identify and describe objects in images by reviewing millions of examples. New, rapidly improving generative AI techniques can create realistic text, images, music and other media.\n",
            "\n",
            "AI programming focuses on cognitive skills that include the following:\n",
            "\n",
            "Learning. This aspect of AI programming focuses on acquiring data and creating rules for how to turn it into actionable information. The rules, which are called algorithms, provide computing devices with step-by-step instructions for how to complete a specific task.\n",
            "Reasoning. This aspect of AI programming focuses on choosing the right algorithm to reach a desired outcome.\n",
            "Self-correction. This aspect of AI programming is designed to continually fine-tune algorithms and ensure they provide the most accurate results possible.\n",
            "Creativity. This aspect of AI uses neural networks, rules-based systems, statistical methods and other AI techniques to generate new images, new text, new music and new ideas.\n",
            "\n",
            "Why is artificial intelligence important?\n",
            "AI is important for its potential to change how we live, work and play. It has been effectively used in business to automate tasks done by humans, including customer service work, lead generation, fraud detection and quality control. In a number of areas, AI can perform tasks much better than humans. Particularly when it comes to repetitive, detail-oriented tasks, such as analyzing large numbers of legal documents to ensure relevant fields are filled in properly, AI tools often complete jobs quickly and with relatively few errors. Because of the massive data sets it can process, AI can also give enterprises insights into their operations they might not have been aware of. The rapidly expanding population of generative AI tools will be important in fields ranging from education and marketing to product design.\n",
            "\n",
            "Indeed, advances in AI techniques have not only helped fuel an explosion in efficiency, but opened the door to entirely new business opportunities for some larger enterprises. Prior to the current wave of AI, it would have been hard to imagine using computer software to connect riders to taxis, but Uber has become a Fortune 500 company by doing just that.\n",
            "\n",
            "AI has become central to many of today's largest and most successful companies, including Alphabet, Apple, Microsoft and Meta, where AI technologies are used to improve operations and outpace competitors. At Alphabet subsidiary Google, for example, AI is central to its search engine, Waymo's self-driving cars and Google Brain, which invented the transformer neural network architecture that underpins the recent breakthroughs in natural language processing.\n",
            "\n",
            "What are the advantages and disadvantages of artificial intelligence?\n",
            "Artificial neural networks and deep learning AI technologies are quickly evolving, primarily because AI can process large amounts of data much faster and make predictions more accurately than humanly possible.\n",
            "\n",
            "While the huge volume of data created on a daily basis would bury a human researcher, AI applications using machine learning can take that data and quickly turn it into actionable information. As of this writing, a primary disadvantage of AI is that it is expensive to process the large amounts of data AI programming requires. As AI techniques are incorporated into more products and services, organizations must also be attuned to AI's potential to cre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "dOruTC7tpneX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ddf7fc-3a2b-49ec-c67a-1a43b854717b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \"$'()+,-./0123456789:;?ABCDEFGHIJKLMNOPRSTUVWZabcdefghijklmnopqrstuvwxyzé\n",
            "75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "id": "hNCC7XO8fEmi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f462650-5ac0-4555-d6f6-a46e70da0825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[55, 56, 56, 1, 67, 55, 52, 65, 52]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Bbp8BYQN1Xo",
        "outputId": "acaa0f7d-fd2f-474a-d4fa-7e2bdfec9642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([28250]) torch.int64\n",
            "tensor([46, 55, 48, 67,  1, 56, 66,  1, 48, 65, 67, 56, 53, 56, 50, 56, 48, 59,\n",
            "         1, 56, 61, 67, 52, 59, 59, 56, 54, 52, 61, 50, 52,  1,  5, 25, 33,  6,\n",
            "        24,  0, 25, 65, 67, 56, 53, 56, 50, 56, 48, 59,  1, 56, 61, 67, 52, 59,\n",
            "        59, 56, 54, 52, 61, 50, 52,  1, 56, 66,  1, 67, 55, 52,  1, 66, 56, 60,\n",
            "        68, 59, 48, 67, 56, 62, 61,  1, 62, 53,  1, 55, 68, 60, 48, 61,  1, 56,\n",
            "        61, 67, 52, 59, 59, 56, 54, 52, 61, 50, 52,  1, 63, 65, 62, 50, 52, 66,\n",
            "        66, 52, 66,  1, 49, 72,  1, 60, 48, 50, 55, 56, 61, 52, 66,  8,  1, 52,\n",
            "        66, 63, 52, 50, 56, 48, 59, 59, 72,  1, 50, 62, 60, 63, 68, 67, 52, 65,\n",
            "         1, 66, 72, 66, 67, 52, 60, 66, 10,  1, 42, 63, 52, 50, 56, 53, 56, 50,\n",
            "         1, 48, 63, 63, 59, 56, 50, 48, 67, 56, 62, 61, 66,  1, 62, 53,  1, 25,\n",
            "        33,  1, 56, 61, 50, 59, 68, 51, 52,  1, 52, 71, 63, 52, 65, 67,  1, 66,\n",
            "        72, 66, 67, 52, 60, 66,  8,  1, 61, 48, 67, 68, 65, 48, 59,  1, 59, 48,\n",
            "        61, 54, 68, 48, 54, 52,  1, 63, 65, 62, 50, 52, 66, 66, 56, 61, 54,  8,\n",
            "         1, 66, 63, 52, 52, 50, 55,  1, 65, 52, 50, 62, 54, 61, 56, 67, 56, 62,\n",
            "        61,  1, 48, 61, 51,  1, 60, 48, 50, 55, 56, 61, 52,  1, 69, 56, 66, 56,\n",
            "        62, 61, 10,  0,  0, 32, 62, 70,  1, 51, 62, 52, 66,  1, 25, 33,  1, 70,\n",
            "        62, 65, 58, 24,  0, 25, 66,  1, 67, 55, 52,  1, 55, 72, 63, 52,  1, 48,\n",
            "        65, 62, 68, 61, 51,  1, 25, 33,  1, 55, 48, 66,  1, 48, 50, 50, 52, 59,\n",
            "        52, 65, 48, 67, 52, 51,  8,  1, 69, 52, 61, 51, 62, 65, 66,  1, 55, 48,\n",
            "        69, 52,  1, 49, 52, 52, 61,  1, 66, 50, 65, 48, 60, 49, 59, 56, 61, 54,\n",
            "         1, 67, 62,  1, 63, 65, 62, 60, 62, 67, 52,  1, 55, 62, 70,  1, 67, 55,\n",
            "        52, 56, 65,  1, 63, 65, 62, 51, 68, 50, 67, 66,  1, 48, 61, 51,  1, 66,\n",
            "        52, 65, 69, 56, 50, 52, 66,  1, 68, 66, 52,  1, 56, 67, 10,  1, 39, 53,\n",
            "        67, 52, 61,  8,  1, 70, 55, 48, 67,  1, 67, 55, 52, 72,  1, 65, 52, 53,\n",
            "        52, 65,  1, 67, 62,  1, 48, 66,  1, 25, 33,  1, 56, 66,  1, 66, 56, 60,\n",
            "        63, 59, 72,  1, 48,  1, 50, 62, 60, 63, 62, 61, 52, 61, 67,  1, 62, 53,\n",
            "         1, 67, 55, 52,  1, 67, 52, 50, 55, 61, 62, 59, 62, 54, 72,  8,  1, 66,\n",
            "        68, 50, 55,  1, 48, 66,  1, 60, 48, 50, 55, 56, 61, 52,  1, 59, 52, 48,\n",
            "        65, 61, 56, 61, 54, 10,  1, 25, 33,  1, 65, 52, 64, 68, 56, 65, 52, 66,\n",
            "         1, 48,  1, 53, 62, 68, 61, 51, 48, 67, 56, 62, 61,  1, 62, 53,  1, 66,\n",
            "        63, 52, 50, 56, 48, 59, 56, 73, 52, 51,  1, 55, 48, 65, 51, 70, 48, 65,\n",
            "        52,  1, 48, 61, 51,  1, 66, 62, 53, 67, 70, 48, 65, 52,  1, 53, 62, 65,\n",
            "         1, 70, 65, 56, 67, 56, 61, 54,  1, 48, 61, 51,  1, 67, 65, 48, 56, 61,\n",
            "        56, 61, 54,  1, 60, 48, 50, 55, 56, 61, 52,  1, 59, 52, 48, 65, 61, 56,\n",
            "        61, 54,  1, 48, 59, 54, 62, 65, 56, 67, 55, 60, 66, 10,  1, 38, 62,  1,\n",
            "        66, 56, 61, 54, 59, 52,  1, 63, 65, 62, 54, 65, 48, 60, 60, 56, 61, 54,\n",
            "         1, 59, 48, 61, 54, 68, 48, 54, 52,  1, 56, 66,  1, 66, 72, 61, 62, 61,\n",
            "        72, 60, 62, 68, 66,  1, 70, 56, 67, 55,  1, 25, 33,  8,  1, 49, 68, 67,\n",
            "         1, 40, 72, 67, 55, 62, 61,  8,  1, 41,  8,  1, 34, 48, 69, 48,  8,  1,\n",
            "        27,  7,  7,  1, 48, 61, 51,  1, 34, 68, 59, 56, 48,  1, 55, 48, 69, 52,\n",
            "         1, 53, 52, 48, 67, 68, 65, 52, 66,  1, 63, 62, 63, 68, 59, 48, 65,  1,\n",
            "        70, 56, 67, 55,  1, 25, 33,  1, 51, 52, 69, 52, 59, 62, 63, 52, 65, 66,\n",
            "        10,  0,  0, 33, 61,  1, 54, 52, 61, 52, 65, 48, 59,  8,  1, 25, 33,  1,\n",
            "        66, 72, 66, 67, 52, 60, 66,  1, 70, 62, 65, 58,  1, 49, 72,  1, 56, 61,\n",
            "        54, 52, 66, 67, 56, 61, 54,  1, 59, 48, 65, 54, 52,  1, 48, 60, 62, 68,\n",
            "        61, 67, 66,  1, 62, 53,  1, 59, 48, 49, 52, 59, 52, 51,  1, 67, 65, 48,\n",
            "        56, 61, 56, 61, 54,  1, 51, 48, 67, 48,  8,  1, 48, 61, 48, 59, 72, 73,\n",
            "        56, 61, 54,  1, 67, 55, 52,  1, 51, 48, 67, 48,  1, 53, 62, 65,  1, 50,\n",
            "        62, 65, 65, 52, 59, 48, 67, 56, 62, 61, 66,  1, 48, 61, 51,  1, 63, 48,\n",
            "        67, 67, 52, 65, 61, 66,  8,  1, 48, 61, 51,  1, 68, 66, 56, 61, 54,  1,\n",
            "        67, 55, 52, 66, 52,  1, 63, 48, 67, 67, 52, 65, 61, 66,  1, 67, 62,  1,\n",
            "        60, 48, 58, 52,  1, 63, 65, 52, 51, 56, 50, 67, 56, 62, 61, 66,  1, 48,\n",
            "        49, 62, 68, 67,  1, 53, 68, 67, 68, 65, 52,  1, 66, 67, 48, 67, 52, 66,\n",
            "        10,  1, 33, 61,  1, 67, 55, 56, 66,  1, 70, 48, 72,  8,  1, 48,  1, 50,\n",
            "        55, 48, 67, 49, 62, 67,  1, 67, 55, 48, 67,  1, 56, 66,  1, 53, 52, 51,\n",
            "         1, 52, 71, 48, 60, 63, 59, 52, 66,  1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "aLWr2jAvN7_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpqtS0R2OAn4",
        "outputId": "52323c89-e023-42d2-ea11-4ee788978446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([46, 55, 48, 67,  1, 56, 66,  1, 48])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hALG73YnOC--",
        "outputId": "066a9b7b-7a44-4ea6-d2f9-2c153c84941d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([46]) the target: 55\n",
            "when input is tensor([46, 55]) the target: 48\n",
            "when input is tensor([46, 55, 48]) the target: 67\n",
            "when input is tensor([46, 55, 48, 67]) the target: 1\n",
            "when input is tensor([46, 55, 48, 67,  1]) the target: 56\n",
            "when input is tensor([46, 55, 48, 67,  1, 56]) the target: 66\n",
            "when input is tensor([46, 55, 48, 67,  1, 56, 66]) the target: 1\n",
            "when input is tensor([46, 55, 48, 67,  1, 56, 66,  1]) the target: 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tDJerDjOFE2",
        "outputId": "dca37e8d-f65e-4b39-e943-b8574460ecde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1, 61, 62, 67, 52, 66,  1, 62],\n",
            "        [48, 59,  1, 50, 62, 61, 69, 52],\n",
            "        [48,  1, 53, 65, 62, 60,  1, 67],\n",
            "        [33,  1, 56, 66,  1, 56, 60, 63]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[61, 62, 67, 52, 66,  1, 62, 65],\n",
            "        [59,  1, 50, 62, 61, 69, 52, 65],\n",
            "        [ 1, 53, 65, 62, 60,  1, 67, 52],\n",
            "        [ 1, 56, 66,  1, 56, 60, 63, 62]])\n",
            "----\n",
            "when input is [1] the target: 61\n",
            "when input is [1, 61] the target: 62\n",
            "when input is [1, 61, 62] the target: 67\n",
            "when input is [1, 61, 62, 67] the target: 52\n",
            "when input is [1, 61, 62, 67, 52] the target: 66\n",
            "when input is [1, 61, 62, 67, 52, 66] the target: 1\n",
            "when input is [1, 61, 62, 67, 52, 66, 1] the target: 62\n",
            "when input is [1, 61, 62, 67, 52, 66, 1, 62] the target: 65\n",
            "when input is [48] the target: 59\n",
            "when input is [48, 59] the target: 1\n",
            "when input is [48, 59, 1] the target: 50\n",
            "when input is [48, 59, 1, 50] the target: 62\n",
            "when input is [48, 59, 1, 50, 62] the target: 61\n",
            "when input is [48, 59, 1, 50, 62, 61] the target: 69\n",
            "when input is [48, 59, 1, 50, 62, 61, 69] the target: 52\n",
            "when input is [48, 59, 1, 50, 62, 61, 69, 52] the target: 65\n",
            "when input is [48] the target: 1\n",
            "when input is [48, 1] the target: 53\n",
            "when input is [48, 1, 53] the target: 65\n",
            "when input is [48, 1, 53, 65] the target: 62\n",
            "when input is [48, 1, 53, 65, 62] the target: 60\n",
            "when input is [48, 1, 53, 65, 62, 60] the target: 1\n",
            "when input is [48, 1, 53, 65, 62, 60, 1] the target: 67\n",
            "when input is [48, 1, 53, 65, 62, 60, 1, 67] the target: 52\n",
            "when input is [33] the target: 1\n",
            "when input is [33, 1] the target: 56\n",
            "when input is [33, 1, 56] the target: 66\n",
            "when input is [33, 1, 56, 66] the target: 1\n",
            "when input is [33, 1, 56, 66, 1] the target: 56\n",
            "when input is [33, 1, 56, 66, 1, 56] the target: 60\n",
            "when input is [33, 1, 56, 66, 1, 56, 60] the target: 63\n",
            "when input is [33, 1, 56, 66, 1, 56, 60, 63] the target: 62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoI35dGDOHnB",
        "outputId": "b235111e-20ef-4126-8bae-ee3ff1cd7290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1, 61, 62, 67, 52, 66,  1, 62],\n",
            "        [48, 59,  1, 50, 62, 61, 69, 52],\n",
            "        [48,  1, 53, 65, 62, 60,  1, 67],\n",
            "        [33,  1, 56, 66,  1, 56, 60, 63]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsSH6wLpOKin",
        "outputId": "c4fb27ad-42c9-40bf-a398-05e6d357a695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 75])\n",
            "tensor(5.0314, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "FmDxF4lGW-$CNbl4jUbbK15o5,2hh4Ar WS:Ens5H++éW\n",
            "géVgpxViAZK7TF5SEd,SéMoBaU05x3;4$J4h\n",
            "MHFaUf-9)\n",
            "iJDkBDk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "KQy8axx8OMoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2iMXsjzOPYV",
        "outputId": "8bfd4370-4cfd-4618-fff1-542a2aace477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.8085246086120605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmIkSbm7OSoz",
        "outputId": "56e6e226-375b-46f3-a6c9-01e1d93a8a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "fUVv0v'p\"eZw4,rEfTw\n",
            "J;guK.Oz34tfvJNWDB676u(9)2B3f4nZ +/ Cr0nZG::ih2;,rNP,q)s2edBJobU,xeiw.'q(t(I5dBG5k:;:F65.'8rCu\n",
            "33-mmz-nO3\n",
            "gNO83M';x3Knctwac?8MfFhCC9kWy)wxRhlIhpo5qéSrk-84sDoR+A3u5e1é:fhcrcW)L37U1+BA,$hEWK(WeGW JaiCmm;\"ei$Ceu75Sanvq37H8 k\"rk?66S-DIo.xF:EUxEx88g-eZ \"OIo'j:yemAeZWd7PhgxqEZO:N11:v/2é\"1hu$z3\n",
            "aBGqi.Bt;EAVJImo $px(oSICe; k;,5,EBpSMdzq5lé8yEVGL6cEJcqwLw/2A+PZO\"$8fcfJuitH\n",
            "yeWAKMku?x'n+too3\"7)SETW T(LCVRpuxWfTwF8hGkRKNGUg9TwdmAH9x8q$éd wd'ONLW)pxd7zAmH3+u?$3LE8gpCViehisiwd.7eD7\"GWVieB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGKI1dsTOU0g",
        "outputId": "7468625b-998f-40e3-bc23-34d36f4878df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3gP8bR9OXmk",
        "outputId": "e2810d10-758e-4004-c245-fde0f62cb3e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ],
      "metadata": {
        "id": "rtBdER4QOZ2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHrHuqrZObqq",
        "outputId": "03d1c22e-4db5-446c-9062-33aef1adc99a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlKBEMsGOdqS",
        "outputId": "0bab24a8-52be-4d21-c339-c59348779609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xPSk4VbOfxS",
        "outputId": "0e026168-aa88-40c9-d7f6-448879f04855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "7KZ-q2hKOiU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPjxeJELOkvQ",
        "outputId": "9221f0e8-ac8e-4b3a-dad2-4d8f6e3a97ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vByjtsk1Omso",
        "outputId": "fbd11715-57f9-42e3-d5ca-66c4334f3f3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQMZaiKwOoxH",
        "outputId": "9b1a83d8-28a6-41de-a437-89f61890973b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GoWHoKaOq4-",
        "outputId": "2df8c604-a792-4ac6-b348-6eb11d8b16e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBrjK8ePOs3h",
        "outputId": "d6ba2ab2-c5e0-4b29-91bd-11bedda760b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4zRosN-Ou9Q",
        "outputId": "da59f891-d81b-43b9-e883-868229c06a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8CeHgQfOxzf",
        "outputId": "482691c2-64b3-49dc-8730-32a8c9afb356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "torch.manual_seed(1337)\n",
        "with open('Myfile.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlfdLXjGOzxF",
        "outputId": "b457b8e6-9a83-49c7-8b5e-34e9e130d1a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.211019 M parameters\n",
            "step 0: train loss 4.5237, val loss 4.5142\n",
            "step 100: train loss 2.6247, val loss 2.6652\n",
            "step 200: train loss 2.4790, val loss 2.5358\n",
            "step 300: train loss 2.3895, val loss 2.4543\n",
            "step 400: train loss 2.2717, val loss 2.3862\n",
            "step 500: train loss 2.1716, val loss 2.3174\n",
            "step 600: train loss 2.0614, val loss 2.2276\n",
            "step 700: train loss 1.9693, val loss 2.1830\n",
            "step 800: train loss 1.8615, val loss 2.0983\n",
            "step 900: train loss 1.8165, val loss 2.0867\n",
            "step 1000: train loss 1.7204, val loss 2.0217\n",
            "step 1100: train loss 1.6660, val loss 1.9975\n",
            "step 1200: train loss 1.5977, val loss 1.9658\n",
            "step 1300: train loss 1.5385, val loss 1.9508\n",
            "step 1400: train loss 1.4587, val loss 1.9475\n",
            "step 1500: train loss 1.4287, val loss 1.9528\n",
            "step 1600: train loss 1.3682, val loss 1.9357\n",
            "step 1700: train loss 1.3329, val loss 1.8974\n",
            "step 1800: train loss 1.2876, val loss 1.9381\n",
            "step 1900: train loss 1.2566, val loss 1.9512\n",
            "step 2000: train loss 1.2275, val loss 1.9706\n",
            "step 2100: train loss 1.1812, val loss 1.9536\n",
            "step 2200: train loss 1.1584, val loss 1.9992\n",
            "step 2300: train loss 1.1290, val loss 2.0076\n",
            "step 2400: train loss 1.0965, val loss 2.0234\n",
            "step 2500: train loss 1.0788, val loss 2.0473\n",
            "step 2600: train loss 1.0541, val loss 2.0807\n",
            "step 2700: train loss 1.0180, val loss 2.0936\n",
            "step 2800: train loss 1.0119, val loss 2.1446\n",
            "step 2900: train loss 0.9742, val loss 2.1276\n",
            "step 3000: train loss 0.9357, val loss 2.1368\n",
            "step 3100: train loss 0.9227, val loss 2.1748\n",
            "step 3200: train loss 0.9163, val loss 2.2277\n",
            "step 3300: train loss 0.8847, val loss 2.2187\n",
            "step 3400: train loss 0.8731, val loss 2.2557\n",
            "step 3500: train loss 0.8514, val loss 2.2880\n",
            "step 3600: train loss 0.8232, val loss 2.2955\n",
            "step 3700: train loss 0.8061, val loss 2.3599\n",
            "step 3800: train loss 0.7954, val loss 2.3865\n",
            "step 3900: train loss 0.7843, val loss 2.3900\n",
            "step 4000: train loss 0.7731, val loss 2.4288\n",
            "step 4100: train loss 0.7368, val loss 2.4973\n",
            "step 4200: train loss 0.7254, val loss 2.5593\n",
            "step 4300: train loss 0.7323, val loss 2.5629\n",
            "step 4400: train loss 0.7012, val loss 2.6073\n",
            "step 4500: train loss 0.6891, val loss 2.6497\n",
            "step 4600: train loss 0.6836, val loss 2.6783\n",
            "step 4700: train loss 0.6552, val loss 2.6343\n",
            "step 4800: train loss 0.6426, val loss 2.7343\n",
            "step 4900: train loss 0.6445, val loss 2.7139\n",
            "step 4999: train loss 0.6417, val loss 2.6973\n",
            "\n",
            "16. The best inning and manalimal beined trainent the data produce capt the 2020 Wenth Chave recent the recent for loans it perial to process that the coulled by poting a prameries. The combinatord Thomains as. Te 2010s, of labely disadvand maring, is designf ammall general Problems integan emforment on Jeop exinserio of malaces and the doof also use se machine use learning in stail-E images general intelligence\n",
            "Some of data process and the caugment beakthroughs in in AI a ran produce new term and machine learning algo's sel in AI and respond to AI descnot expe conself, and servicely suse of gols attenal reirn for to similarities that it the cautive produce that cancessfacture experts of the 20001 ge legal finable of paral detting focuses an chomping stabeiried the folle's abilities of probots are often used to perform to mind beas arre machine aspuounces to inform orgation thnologies are stare in the pandemic and fakes along implements of an images, form old examp robs. Ause AI can pioneeries in the help pan be loE exammer, or a computer program to bealt a computer has intelligence. Newsrouses everving automation. AI beed incorporated its for intelligented intellighes to text can produce application and markent the maning hematle following are some customers to inscreases by a humans workewn inmerocess sations. This type of inderform human being used to automat detect can products. Companing to a many by 1980s, faces to information and marketing complex provide to close anciented assist way itheverwhe repad of design and disrupp texies, but the labeled but, a traine virize problem and the corllect personi dememand range recognition and marketine analytions abeed even shapp ortune the launce that the beat a would basing uses cameral beer pan loabes a dand the data processing algorithms to process the decect not on an eyes, fut lead generalized the doe customer opers AI techniques are used to a chatbot and more by a compponnize cenbate the data falgorithment into mou\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QG1QpsJnO199"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}